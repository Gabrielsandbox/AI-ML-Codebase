{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNm7nyqO4QRvDBBel6WuDr1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gabrielsandbox/AI-ML-Codebase/blob/main/NeuralNetworksFromScratch_Research.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Notes from BloombergGPT talk : https://www.youtube.com/watch?v=m2Scj2SO85Y&ab_channel=TorontoMachineLearningSeries%28TMLS%29\n",
        "\n",
        "# Model size and data size\n",
        "# - model size (number of parameters) and with\n",
        "# - training data size (number of tokens)\n",
        "#\n",
        "# Before the \"Chinchilla paper\" (March 2022)\n",
        "# - Most large language models trained on ~300B tokens\n",
        "# - More compute budget => build a bigger model (same training set)\n",
        "#\n",
        "# The Chinchilla paper investigated  the best way to allocate a given compute budget\n",
        "# between model size and training set size\n",
        "#\n",
        "# Conclusion:\n",
        "# - Most large language model projects should have used smaller models trained on more data\n",
        "# - Optimal tradeoff : if you double model size, you should double training set size\n",
        "#\n",
        "# BloombergGPT : 50B total parameters - 569B data size (in tokens)\n",
        "# * less parameters and more data\n",
        "#\n",
        "#\n",
        "#\n",
        "#"
      ],
      "metadata": {
        "id": "CV56k36ZC4iP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ILydlVdEfav"
      },
      "outputs": [],
      "source": [
        "# EP. 1 https://www.youtube.com/watch?v=Wo5dMEP_BbI&ab_channel=sentdex"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#zip\n",
        "x = [1,2]\n",
        "y = [3,4]\n",
        "\n",
        "for i, j in zip(x,y):\n",
        "  print(i)\n",
        "  print(j)"
      ],
      "metadata": {
        "id": "7Qmpm4pf4iIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Low-level working of a neuron in a neural net\n",
        "#Note: One bias for each different neuron\n",
        "inputs = [1, 2, 3, 2.5]\n",
        "\n",
        "\n",
        "weights = [[0.2, 0.8, -0.5, 1.0],\n",
        "           [0.5, -0.91, 0.26, -0.5],\n",
        "           [-0.26, -0.27, 0.17, 0.87]]\n",
        "\n",
        "biases = [2, 3, 0.5]\n",
        "\n",
        "layer_outputs = []\n",
        "for neuron_weights, neuron_bias in zip(weights,biases):\n",
        "  neuron_output = 0 #Output of given neuron\n",
        "  for neuron_input, weight in zip(inputs, neuron_weights):\n",
        "    neuron_output += neuron_input*weight\n",
        "  neuron_output += neuron_bias\n",
        "  layer_outputs.append(neuron_output)\n",
        "\n",
        "\n",
        "#output = [inputs1[0]*weights[0][0] + inputs1[1]*weights[0][1] + inputs1[2]*weights[0][2] + inputs1[3]*weights[0][3] + biases[0],\n",
        "#          inputs2[0]*weights[1][0] + inputs2[1]*weights[1][1] + inputs2[2]*weights[1][2] + inputs2[3]*weights[1][3] + biases[1],\n",
        "#          inputs3[0]*weights[2][0] + inputs3[1]*weights[2][1] + inputs3[2]*weights[2][2] + inputs3[3]*weights[2][3] + biases[2]]\n",
        "\n",
        "print(layer_outputs)"
      ],
      "metadata": {
        "id": "RH--dLggErVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#zip\n",
        "x = [1,2]\n",
        "y = [3,4]\n",
        "\n",
        "for i, j in zip(x,y):\n",
        "  print(i)\n",
        "  print(j)"
      ],
      "metadata": {
        "id": "7xiGvO1YbNP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The struggle of deep learning is figuring out how to best tune those weights and those biases\n",
        "#Why weights and biases are necessary?\n",
        "#A: One cannot directly edit inputs because they usually come from other functions, but the way to do this is through weights and biases,\n",
        "#by tweaking them we are able to tweak the output of a neuron and the input of the next neuron.\n",
        "\n",
        "#DL definition question : \"How do we adjust weights and biases to get the output values\n",
        "#                          that we want from the fast array of inputs that we might have\""
      ],
      "metadata": {
        "id": "da3Y_diMFwYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#vector = 1D array\n",
        "#Lists of vectors = 2D array\n",
        "#Lists of lists of vectors = 3D array\n",
        "#Tensor = objects that can be represented as array\n",
        "\n",
        "#when it comes to a array >= 2D then it must be homologous in shape.\n",
        "\n",
        "#dot_product/matrix product\n",
        "vector1 = [1,2,3]\n",
        "vector2 = [1,2,3]\n",
        "\n",
        "dot_product = vector1[0] * vector2[0] + vector1[1] * vector2[1] + vector1[2] * vector2[2]\n",
        "print(dot_product) #dot product results in a scalar single value"
      ],
      "metadata": {
        "id": "Y-KujIeHYW0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#dot product of one neuron\n",
        "#Modeling one neuron\n",
        "inputs = [2, 4, 9]\n",
        "weights = [0.2, 3, 4.5]\n",
        "bias = 2\n",
        "\n",
        "dot_product = np.dot(weights, inputs) + bias\n",
        "print(dot_product)"
      ],
      "metadata": {
        "id": "aMBgOdWUbGd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#dot product of a layer of neurons\n",
        "#Modeling 3 neurons\n",
        "inputs = [1, 2, 3, 2.5] # One sample with four features\n",
        "weights = [[0.2, 0.8, -0.5, 1.0],\n",
        "           [0.5, -0.91, 0.26, -0.5],\n",
        "           [-0.26, -0.27, 0.17, 0.87]]\n",
        "\n",
        "biases = [2, 3, 0.5]\n",
        "\n",
        "dot_product = np.dot(weights, inputs) + bias #weights must come first as it is the 2D array, it sorts of dictates what the output array is gonna look like\n",
        "print(dot_product)                           #there is something called the shape problem in deep learning that a lot of beginners have and this essentialy\n",
        "                                             #solves it\n"
      ],
      "metadata": {
        "id": "27TYhMtGcbTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Overfitting hurts generalization when you present all samples at once causing the overfitting to happen\n",
        "#Batch size is usually 24 or 23\n",
        "#weights and biases are associated with individual/unique neurons"
      ],
      "metadata": {
        "id": "2ZqUsKOAetcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Introducing batches\n",
        "import numpy as np\n",
        "\n",
        "inputs = [[1.0, 2.0, 3.0, 2.5],\n",
        "          [2.0, 5.0, -1.0, 2.0],\n",
        "          [-1.5, 2.7, 3.3, -0.8]]\n",
        "\n",
        "weights = [[0.2, 0.8, -0.5, 1.0],\n",
        "           [0.5, -0.91, 0.26, -0.5],\n",
        "           [-0.26, -0.27, 0.17, 0.87]]\n",
        "\n",
        "biases = [2, 3, 0.5]\n",
        "\n",
        "dot_product = np.dot(inputs, np.array(weights).T) + biases\n",
        "print(dot_product)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdtGzo6oZ9IF",
        "outputId": "cce13e5c-f429-4ffb-f476-ec4ef2ba6d19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 4.8    1.21   2.385]\n",
            " [ 8.9   -1.81   0.2  ]\n",
            " [ 1.41   1.051  0.026]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nnfs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJ9NQWjJcHGx",
        "outputId": "ae676227-5625-493a-e8b4-d18c3a106567"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nnfs\n",
            "  Downloading nnfs-0.5.1-py3-none-any.whl (9.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from nnfs) (1.23.5)\n",
            "Installing collected packages: nnfs\n",
            "Successfully installed nnfs-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import nnfs\n",
        "from nnfs.datasets import spiral_data\n",
        "\n",
        "nnfs.init()\n",
        "\n",
        "class Layer_Dense:\n",
        "    def __init__(self, n_inputs, n_neurons):\n",
        "        self.weights = 0.10 * np.random.randn(n_inputs, n_neurons) #randn does a gaussian distribution bounded around 0\n",
        "        self.biases = np.zeros((1, n_neurons))\n",
        "    def forward(self, inputs):\n",
        "        self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "class Activation_ReLU:\n",
        "  def forward(self, inputs):\n",
        "    self.output = np.maximum(0, inputs)\n",
        "\n",
        "class Activation_Softmax:\n",
        "  def forward(self, inputs):\n",
        "    exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True)) #Subtracting by the max value of the batch prevents value overflow as exponentiation happens\n",
        "    probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
        "    self.output = probabilities\n",
        "\n",
        "X, y = spiral_data(samples=100, classes=3)\n",
        "\n",
        "dense1 = Layer_Dense(2, 3)\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "dense2 = Layer_Dense(3, 3)\n",
        "activation2 = Activation_Softmax()\n",
        "\n",
        "dense1.forward(X)\n",
        "activation1.forward(dense1.output)\n",
        "\n",
        "dense2.forward(activation1.output)\n",
        "activation2.forward(dense2.output)\n",
        "\n",
        "print(activation2.output[:5])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cY840pHzh1qU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ep. 5\n",
        "#Activationg Functions\n",
        "# Every neuron has an activation function associated with it\n",
        "# Generally the output layer has a different activation function than the hidden layers\n",
        "#\n",
        "#  - Step activation function\n",
        "#\n",
        "#            {  1 x > 0  }\n",
        "#        y = {           }\n",
        "#            {  0 x <= 0 }\n",
        "#\n",
        "#  - Sigmoid activation function (A little more reliable than Step to train neural net due to the granularity of the output)\n",
        "#    (Has the vanishing gradient problem)\n",
        "#\n",
        "#        y = ___1___\n",
        "#            1 + e^-x\n",
        "#\n",
        "#  - Rectified Linear Unit activation function (ReLU) (It's granular so optimizable and it's fast due to simplicity) (Most popular)\n",
        "#\n",
        "#            {  x x > 0  }\n",
        "#        y = {           }\n",
        "#            {  0 x <= 0 }\n",
        "#\n",
        "#\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "X = [[1.0, 2.0, 3.0, 2.5],\n",
        "     [2.0, 5.0, -1.0, 2.0],\n",
        "     [-1.5, 2.7, 3.3, -0.8]]\n",
        "\n",
        "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\n",
        "output = []\n",
        "\n",
        "#(ReLU)\n",
        "for i in inputs:\n",
        "  if i > 0:\n",
        "    output.append(i)\n",
        "  elif i <= 0:\n",
        "    output.append(0)\n",
        "\n",
        "#another way:\n",
        "# output.append(max(0, i))\n",
        "\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6LpUgV_S2rT",
        "outputId": "b374eaee-6dc5-4a24-941a-ec89715cdd83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to generate a dataset, it is possible to tweak the values and get many different data shapes and complexity\n",
        "\n",
        "def spiral_data(points, classes):\n",
        "    X = np.zeros((points*classes, 2))\n",
        "    y = np.zeros(points*classes, dtype='uint8')\n",
        "    for class_number in range(classes):\n",
        "        ix = range(points*class_number, points*(class_number+1))\n",
        "        r = np.linspace(0.0, 1, points)  # radius\n",
        "        t = np.linspace(class_number*4, (class_number+1)*4, points) + np.random.randn(points)*0.2\n",
        "        X[ix] = np.c_[r*np.sin(t*2.5), r*np.cos(t*2.5)]\n",
        "        y[ix] = class_number\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "51cB4qVJdXJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Softmax Activation Function\n",
        "#Input -> Exponentiate -> Normalize -> Output\n",
        "\n",
        "#Exponentiation Function\n",
        "# It's meant to solve value clipping so to not loose the meaning of negative values\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "layer_outputs = [4.8, 1.21, 2.385]\n",
        "\n",
        "E = math.e\n",
        "\n",
        "exp_layer = np.exp(layer_outputs) #Numpy normally impacts all values in the list\n",
        "print(\"Exponentiated values:\")\n",
        "print(exp_layer)\n",
        "print('')\n",
        "\n",
        "#Normalization\n",
        "# It's a probability distribution of the output layer\n",
        "#    y = ___u____\n",
        "#        Σni=1 ui\n",
        "# Divide the value of one output neuron by the sum of all the output neurons\n",
        "\n",
        "norm_values = exp_layer / np.sum(exp_layer)\n",
        "\n",
        "print(\"Normalized values:\")\n",
        "print(norm_values)\n",
        "print(\"\")\n",
        "print(sum(norm_values))\n",
        "print(\"\")\n",
        "\n",
        "#Up until this point:\n",
        "# Input -> Exponentiate -> Normalize -> Output\n",
        "# That is equal to:\n",
        "# Input -> Softmax -> Output\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8jFtHTG4QST",
        "outputId": "2dbf3733-1c91-47e5-f3b1-c4f5301a47a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exponentiated values:\n",
            "[121.51041752   3.35348465  10.85906266]\n",
            "\n",
            "Normalized values:\n",
            "[0.89528266 0.02470831 0.08000903]\n",
            "\n",
            "0.9999999999999999\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ep. 6 Calculating loss with categorical cross-entropy\n",
        "\n",
        "#Example of loss function: Mean absolute error and it is used with regression\n",
        "#as it gets closer to the value the mean absolute error gets lower and lower\n",
        "\n",
        "#Categorical cross-entropy\n",
        "#One-hot encoding\n",
        "\n",
        "#logarithm(ln()/log):\n",
        "# solving for x -> e ** x = b  (b) is the input to the log / (e) is the eulers number that serves as the base for natural logs\n",
        "'''\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "b = 5.2\n",
        "\n",
        "print(math.e ** np.log(b))\n",
        "print(np.log(b))\n",
        "'''\n",
        "\n",
        "import math\n",
        "\n",
        "softmax_output = [0.7, 0.1, 0.2]\n",
        "target_output = [1, 0, 0]\n",
        "target_class = 0\n",
        "\n",
        "loss = -(math.log(softmax_output[0]) * target_output[0] +\n",
        "         math.log(softmax_output[1]) * target_output[1] +\n",
        "         math.log(softmax_output[2]) * target_output[2])\n",
        "\n",
        "print(loss)\n",
        "loss = -math.log(softmax_output[0])\n",
        "print(loss)\n",
        "\n",
        "#softmax output represents confidence, when confidence raises the loss lowers and when the confidence lowers the loss raises, through this formula"
      ],
      "metadata": {
        "id": "WRBRBOZKph-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nnfs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBEPyILZ8swU",
        "outputId": "c80b84f1-cc7c-44b1-c2b9-36736e5931a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nnfs\n",
            "  Downloading nnfs-0.5.1-py3-none-any.whl (9.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from nnfs) (1.23.5)\n",
            "Installing collected packages: nnfs\n",
            "Successfully installed nnfs-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import nnfs\n",
        "from nnfs.datasets import spiral_data\n",
        "\n",
        "nnfs.init()\n",
        "\n",
        "class Layer_Dense:\n",
        "    def __init__(self, n_inputs, n_neurons):\n",
        "        self.weights = 0.10 * np.random.randn(n_inputs, n_neurons) #randn does a gaussian distribution bounded around 0\n",
        "        self.biases = np.zeros((1, n_neurons))\n",
        "    def forward(self, inputs):\n",
        "        self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "class Activation_ReLU:\n",
        "  def forward(self, inputs):\n",
        "    self.output = np.maximum(0, inputs)\n",
        "\n",
        "class Activation_Softmax:\n",
        "  def forward(self, inputs):\n",
        "    exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True)) #Subtracting by the max value of the batch prevents value overflow as exponentiation happens\n",
        "    probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
        "    self.output = probabilities\n",
        "\n",
        "class Loss:\n",
        "  def calculate(self, output, y):\n",
        "     sample_losses = self.forward(output, y)\n",
        "     data_loss = np.mean(sample_losses)\n",
        "     return data_loss\n",
        "\n",
        "class Loss_CategoricalCrossEntropy(Loss):\n",
        "    def forward(self, y_pred, y_true):\n",
        "       samples = len(y_pred)\n",
        "       y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
        "\n",
        "       if len(y_true.shape) == 1:\n",
        "          correct_confidences = y_pred_clipped[range(samples), y_true]\n",
        "\n",
        "       elif len(y_true.shape) == 2:\n",
        "          correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
        "\n",
        "       negative_log_likehoods = -np.log(correct_confidences)\n",
        "       return negative_log_likehoods\n",
        "\n",
        "\n",
        "X, y = spiral_data(samples=100, classes=3)\n",
        "\n",
        "dense1 = Layer_Dense(2, 3)\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "dense2 = Layer_Dense(3, 3)\n",
        "activation2 = Activation_Softmax()\n",
        "\n",
        "dense1.forward(X)\n",
        "activation1.forward(dense1.output)\n",
        "\n",
        "dense2.forward(activation1.output)\n",
        "activation2.forward(dense2.output)\n",
        "\n",
        "print(activation2.output[:5])\n",
        "\n",
        "loss_function = Loss_CategoricalCrossEntropy()\n",
        "loss = loss_function.calculate(activation2.output, y)\n",
        "\n",
        "print('Loss: ', loss)"
      ],
      "metadata": {
        "id": "qD_KDEa2Emf6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6205044b-5251-450b-b2ab-71ed287ada68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.33333334 0.33333334 0.33333334]\n",
            " [0.33331734 0.3333183  0.33336434]\n",
            " [0.3332888  0.33329153 0.33341965]\n",
            " [0.33325943 0.33326396 0.33347666]\n",
            " [0.33323312 0.33323926 0.33352762]]\n",
            "Loss:  1.098445\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F_jU1a6p8ngP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}