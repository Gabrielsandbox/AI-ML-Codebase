{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwHP842OVgz3dRlZT1z/KJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gabrielsandbox/AI-ML-Codebase/blob/main/FeatureEngineering_Numerical_Transform.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysDPh51lj3_z"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data centering**"
      ],
      "metadata": {
        "id": "gazN23bdkTtg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data centering involves subtracting the mean of a data set from each data point so that the new mean is 0.\n",
        "# This process helps us understand how far above or below each of our data points is from the mean.\n",
        "#\n",
        "#Example:\n",
        "#get the mean of your feature\n",
        "mean_dis = np.mean(distance)\n",
        "\n",
        "#take our distance array and subtract the mean_dis, this will create a new series with the results\n",
        "centered_dis = distance - mean_dis\n",
        "\n",
        "#visualize your new list\n",
        "plt.hist(centered_dis, bins = 5, color = 'g')\n",
        "\n",
        "#label our visual\n",
        "plt.title('Starbucks Distance Data Centered')\n",
        "plt.xlabel('Distance from Mean')\n",
        "plt.ylabel('Count')\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "lVkhmdlekRTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Standardization (also known as Z-Score normalization)**"
      ],
      "metadata": {
        "id": "hsVUYNzhkw-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Center our data, then divide it by the standard deviation.\n",
        "# Once we do that, our entire data set will have a mean of zero and a standard deviation of one.\n",
        "# This allows all of our features to be on the same scale.\n",
        "#\n",
        "#Example:\n",
        "\n",
        "distance = coffee['nearest_starbucks']\n",
        "\n",
        "#find the mean of our feature\n",
        "distance_mean = np.mean(distance)\n",
        "\n",
        "#find the standard deviation of our feature\n",
        "distance_std_dev = np.std(distance)\n",
        "\n",
        "#this will take each data point in distance subtract the mean, then divide by the standard deviation\n",
        "distance_standardized = (distance - distance_mean) / distance_std_dev\n",
        "\n",
        "# print what type distance_standardized is\n",
        "print(type(distance_standardized))\n",
        "#output = <class 'pandas.core.series.Series'>\n",
        "\n",
        "#print the mean\n",
        "print(np.mean(distance_standardized))\n",
        "#output = 7.644158530205996e-17 = close to 0\n",
        "\n",
        "#print the standard deviation\n",
        "print(np.std(distance_standardized))\n",
        "#output = 1.0000000000000013\n",
        "\n",
        "\n",
        "# Note:\n",
        "# This step is critical because some machine learning models will treat all features equally regardless of their scale.\n",
        "# You’ll definitely want to standardize your data in the following situations:\n",
        "\n",
        "# Before Principal Component Analysis\n",
        "# Before using any clustering or distance based algorithm (think KMeans or DBSCAN)\n",
        "# Before KNN\n",
        "# Before performing regularization methods like LASSO and Ridge"
      ],
      "metadata": {
        "id": "Sxq6c2I8kw-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Standardization with Sklearn**"
      ],
      "metadata": {
        "id": "CzX2qSABkx-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Example:\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "reshaped_distance = np.array(distance).reshape(-1,1)\n",
        "distance_scaler = scaler.fit_transform(reshaped_distance)\n",
        "\n",
        "print(np.mean(distance_scaler))\n",
        "#output = -9.464196275493137e-17\n",
        "print(np.std(distance_scaler))\n",
        "#output = 0.9999999999999997"
      ],
      "metadata": {
        "id": "WIbRgU2kkx-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Min-Max Normalization**"
      ],
      "metadata": {
        "id": "XCgR8eVukyZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We find the minimum and maximum data point in our entire data set and set each of those to 0 and 1, respectively.\n",
        "# Then the rest of the data points will transform to a number between 0 and 1,\n",
        "# depending on its distance between the minimum and maximum number.\n",
        "#\n",
        "# This transformation does not work well with data that has extreme outliers.\n",
        "# You will want to perform a min-max normalization if the range between your min and max point is not too drastic.\n",
        "#\n",
        "#Example:\n",
        "\n",
        "distance = coffee['nearest_starbucks']\n",
        "\n",
        "#find the min value in our feature\n",
        "distance_min = np.min(distance)\n",
        "\n",
        "#find the max value in our feature\n",
        "distance_max = np.max(distance)\n",
        "\n",
        "#normalize our feature by following the formula\n",
        "distance_normalized = (distance - distance_min) / (distance_max - distance_min)"
      ],
      "metadata": {
        "id": "tJX293YmkyZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Min-Max Normalization with Sklearn**"
      ],
      "metadata": {
        "id": "nWhr-e_Qpfws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Example:\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "mmscaler = MinMaxScaler()\n",
        "\n",
        "#get our distance feature\n",
        "distance = coffee['nearest_starbucks']\n",
        "\n",
        "#reshape our array to prepare it for the mmscaler\n",
        "reshaped_distance = np.array(distance).reshape(-1,1)\n",
        "\n",
        "#.fit_transform our reshaped data\n",
        "distance_norm = mmscaler.fit_transform(reshaped_distance)\n",
        "\n",
        "#see unique values\n",
        "print(set(np.unique(distance_norm)))\n",
        "#output = {0.0, 0.125, 0.25, 0.375, 0.5, 0.625, 0.75, 0.875, 1.0}"
      ],
      "metadata": {
        "id": "hVAFKjcPkytO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Binning**"
      ],
      "metadata": {
        "id": "oWQ6tNI5povE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Binning data is the process of taking numerical or categorical data and breaking it up into groups.\n",
        "# We could decide to bin our data to help capture patterns in noisy data.\n",
        "\n",
        "# You want to make sure that your bin ranges aren’t so small that your model is still seeing it as noisy data.\n",
        "# Then you also want to make sure that the bin ranges are not so large that your model is unable to pick up on any pattern.\n",
        "# It is a delicate decision to make and will depend on the data you are working with.\n",
        "\n",
        "#First, set the upper boundaries\n",
        "bins = [0, 1, 3, 5, 8.1]\n",
        "\n",
        "coffee['binned_distance'] = pd.cut(coffee['nearest_starbucks'], bins, right = False)\n",
        "\n",
        "print(coffee[['binned_distance', 'nearest_starbucks']].head(3))\n",
        "\n",
        "#output\n",
        "#  binned_distance  nearest_starbucks\n",
        "#0      [5.0, 8.1)                  8\n",
        "#1      [5.0, 8.1)                  8\n",
        "#2      [5.0, 8.1)                  8\n",
        "# Plot the bar graph of binned distances\n",
        "\n",
        "coffee['binned_distance'].value_counts().plot(kind='bar')\n",
        "\n",
        "# Label the bar graph\n",
        "plt.title('Starbucks Distance Distribution')\n",
        "plt.xlabel('Distance')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "# Show the bar graph\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IAuEdJQvq6cA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Natural Log Transformation**"
      ],
      "metadata": {
        "id": "i47o--55kzFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Logarithms are an essential tool in statistical analysis and machine learning preparation.\n",
        "# This transformation works well for right-skewed data and data with large outliers.\n",
        "# After we log transform our data, one large benefit is that it will allow the data to be closer to a “normal” distribution.\n",
        "# It also changes the scale so our data points will drastically reduce the range of their values.\n",
        "\n",
        "# Example:\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "#perform the log transformation\n",
        "log_car = np.log(cars['odometer'])\n",
        "\n",
        "#graph our transformation\n",
        "plt.hist(log_car, bins = 200, color = 'g')\n",
        "\n",
        "#rotate the x labels so we can read it easily\n",
        "plt.xticks(rotation = 45)\n",
        "\n",
        "#provide a title\n",
        "plt.title('Logarithm of Car Odometers')\n",
        "plt.show();\n",
        "\n",
        "# Note:\n",
        "\n",
        "'''\n",
        " When a histogram is right-skewed, where the majority of our data is located on the left side of our graph,\n",
        " if we were to provide this feature to our machine learning model it will see a lot of different data points with readings off on the left of our graph.\n",
        " It will not see a lot of examples with very high readings.\n",
        " This may cause issues with our model, as it may struggle to pick up on patterns that are within those examples off on the right side of our histogram.\n",
        " So we log transform.\n",
        "\n",
        " Using a log transformation in a machine learning model will require some extra interpretation.\n",
        " For example, if you were to log transform your data in a linear regression model,\n",
        " our independent variable has a multiplication relationship with our dependent variable\n",
        " instead of the usual additive relationship we would have if our data was not log-transformed.\n",
        " Keep in mind, just because your data is skewed does not mean that a log transformation is the best answer.\n",
        " You would not want to log transform your feature if:\n",
        " 1 - You have values less than 0. The natural logarithm (which is what we’ve been talking about) of a negative number is undefined.\n",
        " 2 - You have left-skewed data. That data may call for a square or cube transformation.\n",
        " 3 - You have non-parametric data"
      ],
      "metadata": {
        "id": "4F6hXFVoprEJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}